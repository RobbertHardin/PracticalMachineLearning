---
title: 'PML Course Project: Predicting quality, not quantity'
author: "Nomen Nominandum"
date: "30 januari 2016"
output: pdf_document
---

# Introduction
Devices such as _Jawbone Up_, _Nike FuelBand_, and _Fitbit_ help you to collect a large amount of activity data. This data tells you what you did and how often. But it doesn't tell you how well you did it. In this project my goal is to use this data to predict the quality of your activity. 

# Data Acquisition
The data is from accelerometers on the belt, forearm, arm, and dumbell of six young health participants. They were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (_Class A_), throwing the elbows to the front (_Class B_), lifting the dumbbell only halfway (_Class C_), lowering the dumbbell only halfway (_Class D_) and throwing the hips to the front (_Class E_). _Class A_ corresponds to the specified execution of the exercise, while the other four classes correspond to common mistakes. More information is available from [Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har) in the section on the Weight Lifting Exercise (WLE) Dataset: _Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201) Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013._

## Data Preparation
```{r eval = TRUE, echo = FALSE}
if(!file.exists('./training.csv')){
    download.file(url      = 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv',
                  destfile = 'training.csv',
                  method   = 'curl'
                  )
    }
training <- read.csv(file      = './training.csv',
                     na.string = c("", "NA", "#DIV/0!")
                     )
training <- training[ -1 ]

if(!file.exists('./testing.csv')){
    download.file(url = 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv', 
                  destfile = './testing.csv',
                  method   = 'curl'
                  )
    }
testing <- read.csv(file      = './testing.csv',
                    na.string = c("", "NA", "#DIV/0!")
                    )
testing <- testing[ -1 ]
```

The data has already been partitioned in a [training](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and [testing](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv) set. The data was downloaded and imported into R-object `training` resp. `testing` using the R-functions `download.file()` and `read.csv()`. A quick exploraration showed empty strings (`""`) and `NA`'s were used as _NULL_-values. By setting `na.string = c("", "NA")` I solved this when  reading the csv's. The first column of both data files was a rowindex generated by R. I removed it after reading the csv's. `training` now contains 19.622 observation of 159 variables. `testing` now contains 20 observations with an equal number of variables. 

## Exploration
### Health Participants
Both sets have six health participants, their names are _`r paste0(sort(unique(testing$user_name)), collapse = ', ')`_. 

### Correlation
There are also three variables I expect to be correlated: _`r paste0(names(training)[2:4], collapse = ', ')`_. Here's a short example:
```{r}
training[1:6, 1:4]
```
### Variables
```{r eval = FALSE}
# Number of different variables names (nodon)
nodon <- sum(names(training) != names(testing))
# Location of different variable name (lodon)
lodon <- which(names(training) != names(testing))

# Training
names(training)[lodon]
## [1] "classe"
table(training[, 159])
##    A    B    C    D    E 
## 5580 3797 3422 3216 3607 

# Testing
names(testing)[lodon]
## [1] "problem_id"
testing[, 159]
## [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
```
As you can see above, there is one variable that has a different name. It is variable 159. In `training` it's name is `classe` and in `testing` it's name is `problem_id`.  `classe` is the variable we need to predict and I think `problem_id` is used to number the predictions.

Except for the last variable and the first few variables (_`r paste0(names(training)[1:6], collapse = ', ')`_), all variables are combinations of some two dozen words: _`r sort(unique(strsplit(paste0(names(training)[7:158], collapse = '_'), '_')[[1]]))`_. These variables contain the _real_ data, therefor I will remove the first six variables from training  and testing.
```{r eval = TRUE, echo = FALSE}
training <- training[ -(1:6) ]
testing  <- testing[  -(1:6) ]
```
This left me with 53 variables.

### Data Summary
`summary(training)` shows there is a third kind of `NA`: `#DIV/0!`. Now this is a typical [Excel error](https://support.office.com/en-us/article/Correct-a-DIV-0-error-ef9c486d-2fcd-40e6-829d-f36b4d7068b0) you get when you divide by zero. This probably means the data sets were prepared using Excel, not R. Shame on you ;-) Anyhow, I added it to the list of `NA`s and recreated `training` and `testing`.

I am filling the data sets with `NA`s but prediction algortihms don't like missing data, so I remove all columns with no data:
```{r}
training <- training[, colSums(is.na(training)) == 0 ]
testing  <- testing[ , colSums(is.na(testing))  == 0 ]
```

## Data Partitioning
After exploring the data, correcting `NA`s and removing variables, the time has come to split `training` into a training set ( `training_data`, 70%) and a testing set (`testing_data`). To avoid confusion, I will rename `testing` to `validation_data`.
```{r}
suppressMessages(library(caret))
suppressMessages(library(randomForest))
set.seed(37641)
in_training <- createDataPartition(training$classe, 
                                   p    = 0.7,
                                   list = FALSE
                                   )
training_data   <- training[  in_training, ]
testing_data    <- training[ -in_training, ]
validation_data <- testing
```

# Models
The health participants had to perform any of five activities (_A_, ..., _E_). I think there is an underlying physical model, but I don't think it is linear (most physical models aren't). So I start with _Random Forest_ (_RF_). As stated in __Variables__ above, all variables are combinations of some two dozens words. Suggesting there is some structure in the data. So I will also try _Linear Discrimant Analysis_ (_LDA_).

## Random Forrest
At each split Random Forrest chooses a subset of predictors. As there are 52 predictors (the 53rd variable is the one that should be predicted). This leads to a long running analysis. That's why I start with only seven predictors: 
```{r cache = TRUE}
start_RF    <- Sys.time()
model_RF    <- train(classe ~ ., 
                     data   = training_data[, -(8:52)],
                     method = 'rf'
                     )
end_RF      <- Sys.time()
predict_RF  <- predict(model_RF, 
                       newdata = testing_data
                       )

cfMatrix_RF <- confusionMatrix(predict_RF, 
                               testing_data$classe
                               )
accuracy_RF <- cfMatrix_RF$overall[1]

# Show Confusion Matrix
cfMatrix_RF

```
With only seven predictors Random Forrest runs for about 11 minutes and has an accuracy of 0.925. This will do the trick. No need for other models or stacking. I might train the model again with a bigger subset, but with this accuracy I should be able to predict 19 of the 20 questions :-) 

I tested it, and I predicted 19 out of 20 correct. If I had more time, I would run it again wiht more predictors. But the deadline is approaching, so I have to balance my options. 

### Cross Validation
By default Random Forrest uses a 10-fold cross validation. Since it did the job, there is no need to change it.

### Out of Sample Error
With an accuracy of 0.925, the out of sample error is 0.075.

# Prediction 
Using the Random Forrest model from above, I get the following prediction for the validation data (`validation_data` a.k.a. `testing`):
```{r cache = TRUE}
prediction <- predict(model_RF, 
                      newdata = validation_data
                      )
## -(8:52)
##  [1] B A B A A E D B A A B A B A E E A B B B
## Levels: A B C D E
```

